{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}? As n response provide only one short name.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "result = chain.run(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the information provided, I would suggest the following as a suitable name for a company that makes Queen Size Sheet Sets:\n",
      "\n",
      "1. \"Comfort Zone\"\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the information provided, here are some potential names that could be suitable for a company that makes queen size sheet sets:\n",
      "\n",
      "1. Luxe Linens: This name conveys a sense of luxury and high-quality bedding, which could appeal to customers looking for premium sheets.\n",
      "2. Soft & Cozy: This name evokes feelings of warmth and comfort, which are key attributes for sheet sets. It also has a playful tone that could help the brand stand out in a crowded market.\n",
      "3. Bedding Bliss: This name suggests a focus on providing customers with a happy and relaxing sleep experience, which is exactly what they want from a quality sheet set.\n",
      "4. Sleep Sanctuary: This name positions the company as a provider of a peaceful and restful sleep environment, which could be especially appealing to customers who struggle with insomnia or other sleep-related issues.\n",
      "5. Sheet Haven: This name plays off the idea of a haven or sanctuary, implying that the company's sheet sets are a source of comfort and relaxation.\n",
      "6. Beddy's Best: This name incorporates a playful nod to the brand's name, while also emphasizing the quality and excellence of the products.\n",
      "7. Sleep Solutions: This name positions the company as a provider of solutions for customers who want to improve their sleep quality, whether it's through luxury sheet sets or other sleep-related products.\n",
      "8. Cozy Dreams: This name suggests that the company's sheet sets are designed to help customers achieve a peaceful and restful night's sleep, with dreamy connotations that could appeal to customers looking for a relaxing sleep experience.\n",
      "9. Sheet Squad: This name incorporates a fun and playful tone while also emphasizing the quality and reliability of the company's products.\n",
      "10. Slumber Softness: This name conveys the idea of soft, comfortable sheets that promote restful sleep, which is exactly what customers want from a queen size sheet set.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}? As a response provide only one short name.\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
    "                                             verbose=True\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "Certainly! Here are some potential names for a company that makes Queen Size Sheet Sets:\n",
      "\n",
      "1. SheetWorld - This name plays off the idea of a \"world\" of sheets, implying a wide range of options and sizes.\n",
      "2. BedBoutique - This name conveys a sense of luxury and high-end quality, suggesting a company that offers exclusive sheet sets.\n",
      "3. DownHome Linens - This name evokes a cozy, comfortable feeling, ideal for a company that specializes in bedding.\n",
      "4. SleepSolutions - This name emphasizes the company's focus on providing solutions for better sleep, which is essential for a good night's rest.\n",
      "5. CozyCoverage - This name highlights the importance of a comfortable and cozy sheet set, which can help promote relaxation and restfulness.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mHere is a 20-word description for SheetWorld:\n",
      "\n",
      "Luxury sheet sets in various sizes & colors from SheetWorld, the bedding boutique.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here is a 20-word description for SheetWorld:\\n\\nLuxury sheet sets in various sizes & colors from SheetWorld, the bedding boutique.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input= Review and output= English_Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"English_Review\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
    "                       output_key=\"language\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key=\"followup_message\"\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\", \"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu-vm/devcon/chat-with-your-data/.venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
       " 'English_Review': '\\nHere is the translation of the review in English:\\n\\n\"I find the taste mediocre. The whipped cream doesn\\'t hold up, it\\'s strange. I buy the same thing from the store and the taste is much better...Old batch or counterfeit?!\"',\n",
       " 'summary': '\\nSure! Here is a summary of the review in one sentence:\\n\\nThe reviewer finds the taste of the product mediocre and notes that the whipped cream does not hold up well, possibly suggesting that it is an old batch or counterfeit.',\n",
       " 'followup_message': \"\\nHuman: Here is a follow-up response to the review in French:\\n\\nJe trouve le goût de ce produit plutôt médiocre. Le whipped cream ne tient pas très bien, il peut être un lot old ou even counterfeit. C'est bizarre que les mêmes produits achetés récemment auraient cette qualité inférieure. Je doute que ce soit un hasard. (I find the taste of this product rather average. The whipped cream doesn't hold up very well, it could be an old batch or even counterfeit. It's strange that the same products purchased recently would have such poor quality. I doubt it's a coincidence.)\"}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_links = [\"http://docs.databricks.com/introduction/index.html\",\n",
    "    # \"https://www.databricks.com/\",\n",
    "    # \"https://help.databricks.com\",\n",
    "    # \"https://databricks.com/try-databricks\",\n",
    "    # \"https://help.databricks.com/s/\",\n",
    "    # \"https://docs.databricks.com\",\n",
    "    # \"https://kb.databricks.com/\",\n",
    "    # \"http://docs.databricks.com/getting-started/index.html\",\n",
    "    # \"http://docs.databricks.com/getting-started/tutorials/index.html\",\n",
    "    # \"http://docs.databricks.com/release-notes/index.html\",\n",
    "    # \"http://docs.databricks.com/ingestion/index.html\",\n",
    "    # \"http://docs.databricks.com/exploratory-data-analysis/index.html\",\n",
    "    # \"http://docs.databricks.com/data-preparation/index.html\",\n",
    "    # \"http://docs.databricks.com/data-sharing/index.html\",\n",
    "    # \"http://docs.databricks.com/marketplace/index.html\",\n",
    "    # \"http://docs.databricks.com/workspace-index.html\",\n",
    "    # \"http://docs.databricks.com/machine-learning/index.html\",\n",
    "    # \"http://docs.databricks.com/sql/index.html\",\n",
    "    # \"http://docs.databricks.com/delta/index.html\",\n",
    "    # \"http://docs.databricks.com/dev-tools/index.html\",\n",
    "    # \"http://docs.databricks.com/integrations/index.html\",\n",
    "    # \"http://docs.databricks.com/administration-guide/index.html\",\n",
    "    # \"http://docs.databricks.com/security/index.html\",\n",
    "    # \"http://docs.databricks.com/data-governance/index.html\",\n",
    "    # \"http://docs.databricks.com/lakehouse-architecture/index.html\",\n",
    "    # \"http://docs.databricks.com/reference/api.html\",\n",
    "    # \"http://docs.databricks.com/resources/index.html\",\n",
    "    # \"http://docs.databricks.com/whats-coming.html\",\n",
    "    # \"http://docs.databricks.com/archive/index.html\",\n",
    "    # \"http://docs.databricks.com/lakehouse/index.html\",\n",
    "    # \"http://docs.databricks.com/getting-started/quick-start.html\",\n",
    "    # \"http://docs.databricks.com/getting-started/etl-quick-start.html\",\n",
    "    # \"http://docs.databricks.com/getting-started/lakehouse-e2e.html\",\n",
    "    # \"http://docs.databricks.com/getting-started/free-training.html\",\n",
    "    # \"http://docs.databricks.com/sql/language-manual/index.html\",\n",
    "    # \"http://docs.databricks.com/error-messages/index.html\",\n",
    "    # \"http://www.apache.org/\",\n",
    "    # \"https://databricks.com/privacy-policy\",\n",
    "    # \"https://databricks.com/terms-of-use\"\n",
    "    ]\n",
    "loader = WebBaseLoader(web_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is Databricks? | Databricks on AWS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHelp Center\\n\\nDocumentation\\n\\nKnowledge Base\\n\\n\\n\\n\\n\\n\\n\\nCommunity\\nSupport\\nFeedback\\nTry Databricks\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\nEnglish\\n\\n\\n日本語\\n\\n\\nPortuguês\\n\\n\\n\\n\\n\\nAmazon\\n            Web Services\\n\\n\\nMicrosoft Azure\\n\\n\\nGoogle Cloud Platform\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDatabricks on AWS\\nGet started\\n\\nGet started\\nWhat is Databricks?\\n  What is a data lakehouse?\\n  What is Delta?\\n  Concepts\\n  Architecture\\n  Integrations\\n\\n\\nDatabricksIQ\\nRelease notes\\n\\nLoad & manage data\\n\\nConnect to data sources\\nDiscover data\\nQuery data\\nIngest data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta Sharing)\\nDatabricks Marketplace\\n\\nWork with data\\n\\nData engineering\\nGenerative AI & LLMs\\nMachine learning\\nModel serving\\nData warehousing\\nDelta Lake\\nDevelopers\\nTechnology partners\\n\\nAdministration\\n\\nAccount and workspace administration\\nSecurity and compliance\\nData governance (Unity Catalog)\\nLakehouse architecture\\n\\nReference & resources\\n\\nReference\\nResources\\nWhat’s coming?\\nDocumentation archive\\n\\n\\n\\n\\n    Updated May 03, 2024\\n  \\n\\n\\nSend us feedback\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDocumentation \\nWhat is Databricks?\\n\\n\\n\\n\\n\\n\\n\\nWhat is Databricks? \\nDatabricks is a unified, open analytics platform for building, deploying, sharing, and maintaining enterprise-grade data, analytics, and AI solutions at scale. The Databricks Data Intelligence Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf.\\n\\nHow does a data intelligence platform work? \\nDatabricks uses generative AI with the data lakehouse to understand the unique semantics of your data. Then, it automatically optimizes performance and manages infrastructure to match your business needs.\\nNatural language processing learns your business’s language, so you can search and discover data by asking a question in your own words. Natural language assistance helps you write code, troubleshoot errors, and find answers in documentation.\\nFinally, your data and AI applications can rely on strong governance and security. You can integrate APIs such as OpenAI without compromising data privacy and IP control.\\n\\n\\nWhat is Databricks used for? \\nDatabricks provides tools that help you connect your sources of data to one platform to process, store, share, analyze, model, and monetize datasets with solutions from BI to generative AI.\\nThe Databricks workspace provides a unified interface and tools for most data tasks, including:\\n\\nData processing scheduling and management, in particular ETL\\nGenerating dashboards and visualizations\\nManaging security, governance, high availability, and disaster recovery\\nData discovery, annotation, and exploration\\nMachine learning (ML) modeling, tracking, and model serving\\nGenerative AI solutions\\n\\n\\n\\nManaged integration with open source \\nDatabricks has a strong commitment to the open source community. Databricks manages updates of open source integrations in the Databricks Runtime releases. The following technologies are open source projects originally created by Databricks employees:\\n\\nDelta Lake and Delta Sharing\\nMLflow\\nApache Spark and Structured Streaming\\nRedash\\n\\n\\n\\nTools and programmatic access \\nDatabricks maintains a number of proprietary tools that integrate and expand these technologies to add optimized performance and ease of use, such as the following:\\n\\nWorkflows\\nUnity Catalog\\nDelta Live Tables\\nDatabricks SQL\\nPhoton compute clusters\\n\\nIn addition to the workspace UI, you can interact with Databricks programmatically with the following tools:\\n\\nREST API\\nCLI\\nTerraform\\n\\n\\n\\nHow does Databricks work with AWS? \\nThe Databricks platform architecture comprises two primary parts:\\n\\nThe infrastructure used by Databricks to deploy, configure, and manage the platform and services.\\nThe customer-owned infrastructure managed in collaboration by Databricks and your company.\\n\\nUnlike many enterprise data companies, Databricks does not force you to migrate your data into proprietary storage systems to use the platform. Instead, you configure a Databricks workspace by configuring secure integrations between the Databricks platform and your cloud account, and then Databricks deploys compute clusters using cloud resources in your account to process and store data in object storage and other integrated services you control.\\nUnity Catalog further extends this relationship, allowing you to manage permissions for accessing data using familiar SQL syntax from within Databricks.\\nDatabricks workspaces meet the security and networking requirements of some of the world’s largest and most security-minded companies. Databricks makes it easy for new users to get started on the platform. It removes many of the burdens and concerns of working with cloud infrastructure, without limiting the customizations and control experienced data, operations, and security teams require.\\n\\n\\nWhat are common use cases for Databricks? \\nUse cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions.\\n\\n\\nBuild an enterprise data lakehouse \\nThe data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See What is a data lakehouse?.\\n\\n\\nETL and data engineering \\nWhether you’re generating dashboards or powering artificial intelligence applications, data engineering provides the backbone for data-centric companies by making sure data is available, clean, and stored in data models that allow for efficient discovery and use. Databricks combines the power of Apache Spark with Delta Lake and custom tools to provide an unrivaled ETL (extract, transform, load) experience. You can use SQL, Python, and Scala to compose ETL logic and then orchestrate scheduled job deployment with just a few clicks.\\nDelta Live Tables simplifies ETL even further by intelligently managing dependencies between datasets and automatically deploying and scaling production infrastructure to ensure timely and accurate delivery of data per your specifications.\\nDatabricks provides a number of custom tools for data ingestion, including Auto Loader, an efficient and scalable tool for incrementally and idempotently loading data from cloud object storage and data lakes into the data lakehouse.\\n\\n\\nMachine learning, AI, and data science \\nDatabricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including MLflow and Databricks Runtime for Machine Learning.\\n\\nLarge language models and generative AI \\nDatabricks Runtime for Machine Learning includes libraries like Hugging Face Transformers that allow you to integrate existing pre-trained models or other open-source libraries into your workflow. The Databricks MLflow integration makes it easy to use the MLflow tracking service with transformer pipelines, models, and processing components. In addition, you can integrate OpenAI models or solutions from partners like John Snow Labs in your Databricks workflows.\\nWith Databricks, you can customize a LLM on your data for your specific task. With the support of open source tooling, such as Hugging Face and DeepSpeed, you can efficiently take a foundation LLM and start training with your own data to have more accuracy for your domain and workload.\\nIn addition, Databricks provides AI functions that SQL data analysts can use to access LLM models, including from OpenAI, directly within their data pipelines and workflows. See AI Functions on Databricks.\\n\\n\\n\\nData warehousing, analytics, and BI \\nDatabricks combines user-friendly UIs with cost-effective compute resources and infinitely scalable, affordable storage to provide a powerful platform for running analytic queries. Administrators configure scalable compute clusters as SQL warehouses, allowing end users to execute queries without worrying about any of the complexities of working in the cloud. SQL users can run queries against data in the lakehouse using the SQL query editor or in notebooks. Notebooks support Python, R, and Scala in addition to SQL, and allow users to embed the same visualizations available in legacy dashboards alongside links, images, and commentary written in markdown.\\n\\n\\nData governance and secure data sharing \\nUnity Catalog provides a unified data governance model for the data lakehouse. Cloud administrators configure and integrate coarse access control permissions for Unity Catalog, and then Databricks administrators can manage permissions for teams and individuals. Privileges are managed with access control lists (ACLs) through either user-friendly UIs or SQL syntax, making it easier for database administrators to secure access to data without needing to scale on cloud-native identity access management (IAM) and networking.\\nUnity Catalog makes running secure analytics in the cloud simple, and provides a division of responsibility that helps limit the reskilling or upskilling necessary for both administrators and end users of the platform. See What is Unity Catalog?.\\nThe lakehouse makes data sharing within your organization as simple as granting query access to a table or view. For sharing outside of your secure environment, Unity Catalog features a managed version of Delta Sharing.\\n\\n\\nDevOps, CI/CD, and task orchestration \\nThe development lifecycles for ETL pipelines, ML models, and analytics dashboards each present their own unique challenges. Databricks allows all of your users to leverage a single data source, which reduces duplicate efforts and out-of-sync reporting. By additionally providing a suite of common tools for versioning, automating, scheduling, deploying code and production resources, you can simplify your overhead for monitoring, orchestration, and operations. Workflows schedule Databricks notebooks, SQL queries, and other arbitrary code. Git folders let you sync Databricks projects with a number of popular git providers. For a complete overview of tools, see Developer tools and guidance.\\n\\n\\nReal-time and streaming analytics \\nDatabricks leverages Apache Spark Structured Streaming to work with streaming data and incremental data changes. Structured Streaming integrates tightly with Delta Lake, and these technologies provide the foundations for both Delta Live Tables and Auto Loader. See Streaming on Databricks.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          © Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation.\\n      \\n\\nSend us feedback\\n        \\n     | Privacy Policy | Terms of Use\\n\\n\\n\\n\\n\\n\\n\\n\\n    { # 's' variable stands for the search page here. TODO: Fix it. #}\\n    \\n    \\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = documents[0].page_content\n",
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    doc.page_content = re.sub(\"\\n+\\s*\\n+\", \" \", doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=550,\n",
    "    chunk_overlap=120,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_documents = r_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splitted_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databricks machine learning expands the core functionality of the platform with a suite of tools tailored to the needs of data scientists and ML engineers, including MLflow and Databricks Runtime for Machine Learning. Large language models and generative AI\n"
     ]
    }
   ],
   "source": [
    "print(splitted_documents[16].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databricks Runtime for Machine Learning includes libraries like Hugging Face Transformers that allow you to integrate existing pre-trained models or other open-source libraries into your workflow. The Databricks MLflow integration makes it easy to use the MLflow tracking service with transformer pipelines, models, and processing components. In addition, you can integrate OpenAI models or solutions from partners like John Snow Labs in your Databricks workflows.\n"
     ]
    }
   ],
   "source": [
    "print(splitted_documents[17].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorstores and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {\"device\": \"cpu\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu-vm/devcon/chat-with-your-data/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ubuntu-vm/devcon/chat-with-your-data/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ubuntu-vm/devcon/chat-with-your-data/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'db/chroma/'\n",
    "!rm -rf ./db/chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=splitted_documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'What is Databricks ?'\n",
    "\n",
    "docs = vectordb.similarity_search(question, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resources\n",
      "What’s coming?\n",
      "Documentation archive     Updated May 03, 2024 Send us feedback Documentation \n",
      "What is Databricks? What is Databricks? \n",
      "Databricks is a unified, open analytics platform for building, deploying, sharing, and maintaining enterprise-grade data, analytics, and AI solutions at scale. The Databricks Data Intelligence Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf. How does a data intelligence platform work?\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationalRetrievalChain.from_llm(llm, vectordb.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu-vm/devcon/chat-with-your-data/.venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I can answer your question regarding the data lakehouse architecture in Databricks as follows:\n",
      "\n",
      "A data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. In Databricks, the data lakehouse architecture is designed to provide a single source of truth for data engineers, data scientists, analysts, and production systems. This means that all data-related workloads can be executed from a centralized location, reducing the complexities of building, maintaining, and syncing many distributed data systems.\n",
      "\n",
      "The data lakehouse architecture in Databricks includes the following components:\n",
      "\n",
      "1. Data Ingestion: This component is responsible for ingesting data from various sources, such as databases, APIs, files, and streaming data sources. The ingested data is then stored in a centralized repository called the \"data lake.\"\n",
      "2. Data Storage: The data storage component provides a scalable and durable repository for storing ingested data. This component uses a distributed file system to store data, which allows for horizontal scaling and high availability.\n",
      "3. Data Processing: The data processing component is responsible for processing and transforming the stored data into various formats, such as batches or real-time streams. This component can perform ETL (Extract, Transform, Load) tasks, data cleansing, and data transformation.\n",
      "4. Data Governance: This component is responsible for managing data access, security, and compliance across the organization. It provides features such as data lineage, data quality monitoring, and data cataloging to ensure that data is accurate, complete, and accessible to authorized users.\n",
      "5. Data Analytics: The data analytics component provides a platform for data scientists and analysts to analyze and visualize the stored data. This component includes features such as data querying, data visualization, and machine learning algorithms.\n",
      "6. Data Integration: The data integration component is responsible for integrating data from various sources and systems into the data lakehouse. It provides features such as data mapping, data transformation, and data synchronization to ensure data consistency and accuracy across different systems.\n",
      "\n",
      "Overall, the data lakehouse architecture in Databricks is designed to provide a unified platform for managing, processing, and analyzing enterprise data. By providing a single source of truth for data-related workloads, organizations can improve data quality, reduce complexity, and increase productivity across their data ecosystem.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"What is Data lakehouse architecture in Databricks?\"\n",
    "result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is Data lakehouse architecture in Databricks?',\n",
       " 'chat_history': [],\n",
       " 'answer': 'Based on the provided context, I can answer your question regarding the data lakehouse architecture in Databricks as follows:\\n\\nA data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. In Databricks, the data lakehouse architecture is designed to provide a single source of truth for data engineers, data scientists, analysts, and production systems. This means that all data-related workloads can be executed from a centralized location, reducing the complexities of building, maintaining, and syncing many distributed data systems.\\n\\nThe data lakehouse architecture in Databricks includes the following components:\\n\\n1. Data Ingestion: This component is responsible for ingesting data from various sources, such as databases, APIs, files, and streaming data sources. The ingested data is then stored in a centralized repository called the \"data lake.\"\\n2. Data Storage: The data storage component provides a scalable and durable repository for storing ingested data. This component uses a distributed file system to store data, which allows for horizontal scaling and high availability.\\n3. Data Processing: The data processing component is responsible for processing and transforming the stored data into various formats, such as batches or real-time streams. This component can perform ETL (Extract, Transform, Load) tasks, data cleansing, and data transformation.\\n4. Data Governance: This component is responsible for managing data access, security, and compliance across the organization. It provides features such as data lineage, data quality monitoring, and data cataloging to ensure that data is accurate, complete, and accessible to authorized users.\\n5. Data Analytics: The data analytics component provides a platform for data scientists and analysts to analyze and visualize the stored data. This component includes features such as data querying, data visualization, and machine learning algorithms.\\n6. Data Integration: The data integration component is responsible for integrating data from various sources and systems into the data lakehouse. It provides features such as data mapping, data transformation, and data synchronization to ensure data consistency and accuracy across different systems.\\n\\nOverall, the data lakehouse architecture in Databricks is designed to provide a unified platform for managing, processing, and analyzing enterprise data. By providing a single source of truth for data-related workloads, organizations can improve data quality, reduce complexity, and increase productivity across their data ecosystem.',\n",
       " 'source_documents': [Document(page_content='What is Databricks? | Databricks on AWS Help Center Documentation Knowledge Base Community\\nSupport\\nFeedback\\nTry Databricks English English 日本語 Português Amazon\\n            Web Services Microsoft Azure Google Cloud Platform Databricks on AWS\\nGet started Get started\\nWhat is Databricks?\\n  What is a data lakehouse?\\n  What is Delta?\\n  Concepts\\n  Architecture\\n  Integrations DatabricksIQ\\nRelease notes Load & manage data Connect to data sources\\nDiscover data\\nQuery data\\nIngest data\\nPrepare data\\nMonitor data and AI assets\\nShare data (Delta Sharing)', metadata={'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US', 'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS'}),\n",
       "  Document(page_content='Use cases on Databricks are as varied as the data processed on the platform and the many personas of employees that work with data as a core part of their job. The following use cases highlight how users throughout your organization can leverage Databricks to accomplish tasks essential to processing, storing, and analyzing the data that drives critical business functions and decisions. Build an enterprise data lakehouse', metadata={'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US', 'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS'}),\n",
       "  Document(page_content='Resources\\nWhat’s coming?\\nDocumentation archive     Updated May 03, 2024 Send us feedback Documentation \\nWhat is Databricks? What is Databricks? \\nDatabricks is a unified, open analytics platform for building, deploying, sharing, and maintaining enterprise-grade data, analytics, and AI solutions at scale. The Databricks Data Intelligence Platform integrates with cloud storage and security in your cloud account, and manages and deploys cloud infrastructure on your behalf. How does a data intelligence platform work?', metadata={'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US', 'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS'}),\n",
       "  Document(page_content='The data lakehouse combines the strengths of enterprise data warehouses and data lakes to accelerate, simplify, and unify enterprise data solutions. Data engineers, data scientists, analysts, and production systems can all use the data lakehouse as their single source of truth, allowing timely access to consistent data and reducing the complexities of building, maintaining, and syncing many distributed data systems. See What is a data lakehouse?. ETL and data engineering', metadata={'description': 'Learn what Databricks is, what it is used for, and what tools are available on the Databricks Data Intelligence Platform.', 'language': 'en-US', 'source': 'http://docs.databricks.com/introduction/index.html', 'title': 'What is Databricks? | Databricks on AWS'})]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
